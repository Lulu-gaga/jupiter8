#!/usr/bin/env python
# Standard imports
import os
import argparse
import logging
import json
import datetime
import yaml
import shutil

# External dependencies
import pymongo
import pandas as pd
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from keras.models import Model
from keras.optimizers import SGD, Adadelta
from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras.callbacks import CSVLogger, LearningRateScheduler
from keras.utils import to_categorical

# Local imports
from data.load2 import load_images, get_training_generator
from utils.argutils import valid_path
from aconv import get_aconv
from small import get_model
from training.learning import step_decay, get_lr_metric

DATABASE = 'mstar2'
COLLECTION = 'targets'
SYNTHETICS = 'synthetics'
DEP_ANGLE_TRAIN = '17_DEG'
LABEL_MAP_FILE = 'label_map.json'
DEFAULT_SAVE_PERIOD = 5
DEFAULT_EPOCHS = 50
DEFAULT_BATCH_SIZE = 32
DEFAULT_VERBOSITY = 0
DEFAULT_VALIDATION_SPLIT = 0.1
DEFAULT_LEARN_RATE = 0.001
CHECKPOINT_FILEPATH = "weights.{epoch:02d}.h5"
TRAINING_LOG = 'training.log'
CSV_FILE = 'train_samples.csv'

log = logging.getLogger(__name__)

def get_parser():
    parser = argparse.ArgumentParser(
        description="""Train a Deep Convolutional model to classify the MSTAR
                       dataset""",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('configfn', type=valid_path,
        help="""Path to train_mstar config file to use""")
    parser.add_argument('--debug', action='store_true',
        help="""Enable debug statements""")
    return parser


def get_callbacks(outdir, learn_rate=0.01, verbosity=0, save_period=5):
    checkpoint = ModelCheckpoint(filepath=os.path.join(outdir, CHECKPOINT_FILEPATH),
                                 monitor='val_loss',
                                 verbose=verbosity,
                                 save_best_only=True,
                                 period=save_period)
    csv_logger = CSVLogger(filename=os.path.join(outdir, TRAINING_LOG),
                           separator=',',
                           append=False)
    return [checkpoint, csv_logger]


def main(args):
    #
    # Parse config
    #
    with open(args.configfn, 'r') as f:
        config = yaml.load(f)
    prefix = config['setup']['prefix']
    outdir = config['setup']['outdir']
    if not os.path.exists(outdir):
        raise ValueError("Output directory %s does not exist" % outdir)

    # Make subdirectory for session
    tstamp = datetime.datetime.utcnow()
    subdir = tstamp.strftime("%Y%m%d%H%M%S")
    subdir = '.'.join(('train', subdir))
    if prefix:
        subdir = '_'.join((prefix, subdir))
    subdir = os.path.join(outdir, subdir)
    os.mkdir(os.path.join(outdir, subdir), 0o755)
    log.info("Results will be logged to %s" % subdir)

    # Save copy of config file in subdirectory
    shutil.copy(args.configfn, os.path.join(subdir, os.path.basename(args.configfn)))
   
    # 
    # Setup DB connection
    #
    client = pymongo.MongoClient()
    db = client[config['db']['database']]
    real_collection = db[config['db']['real_collection']]
    synth_collection = db[config['db']['synth_collection']]

    #
    # Construct dataframe of synthetic training samples
    #
    synth_sources = [] # Container for real samples used to create synthetics
    if config['synthetics']['use_synthetics']:
        log.info("Loading synthetic samples")
        cursor = synth_collection.find() # pull all synths
        df = pd.DataFrame(list(cursor))
        #
        # Filter synthetics based on number of samples the GAN was trained with
        #
        num_trained = config['synthetics']['num_samples_trained_with']
        log.info("Pulling synthetics for num_samples_trained_with = %d" % num_trained)
        if not num_trained:
            synth_df = df[df.all_training_samples_used == True]
        else:
            log.info("Limiting synthetics to those trained with %d samples" % num_trained)
            if num_trained not in df.num_samples_trained_with.unique():
                raise ValueError("%d not a valid value for num_samples_trained_with"
                    % num_trained)
            synth_df = df[df.num_samples_trained_with == num_trained]
        log.debug("Num synthetic samples: %d" % len(synth_df))
        #
        # Filter number of synthetics used
        #
        num_synths = config['synthetics']['num_synth_samples']
        if num_synths:
            log.info("Limiting synth samples per class to %d" % num_synths)
            subsets = []
            for name, group in synth_df.groupby('target_class'):
                subsets.append(group.sample(num_synths))
            synth_df = pd.concat(subsets)
            log.debug("Num synthetic samples: %d" % len(synth_df))
        #
        # Get list of all real samples used to generate the synthetics
        #
        print(synth_df.sources.unique())
        for sourcefn in synth_df.sources.unique():
            with open(sourcefn, 'r') as f:
                s = yaml.load(f)
            synth_sources.extend(s['filenames'])
        log.debug("Total number of synthetic sources %d" % len(synth_sources))
    #
    # Construct dataframe of real training samples
    #
    log.info("Loading real samples")
    query = {'depression_angle': DEP_ANGLE_TRAIN}
    cursor = real_collection.find(query)
    df = pd.DataFrame(list(cursor))
    log.debug("Num real samples: %d" % len(df))
    if synth_sources:
        log.info("Excluding real samples used in synthetics generation")
        df = df[df.filename.apply(lambda x: x not in synth_sources)]
        log.debug("Num real samples: %d" % len(df))
    max_samples = config['training']['max_samples']
    if max_samples: # Limit smaples per class
        log.info("Limiting max real samples per class to %d" % max_samples)
        subsets = []
        for name, group in df.groupby('target_class'):
            subsets.append(group.sample(max_samples))
        df = pd.concat(subsets)
    # Combine data frames
    if config['synthetics']['use_synthetics']:
        df = pd.concat([df, synth_df])

    # Write dataframe to CSV file
    df.to_csv(os.path.join(subdir, CSV_FILE))

    # Load images
    images = load_images(list(df.filename), crop_dim=(128,128))

    # Encode labels - for categogorical crossentropy the labels must be
    # one hot encoded
    classes = df.target_class.unique()
    classes.sort()
    num_classes = len(classes)
    label_map = dict(zip(classes, range(num_classes)))
    labels = df.target_class.apply(lambda x: label_map[x])
    labels = to_categorical(labels, num_classes)
    
    # Write out label map
    with open(os.path.join(subdir, LABEL_MAP_FILE), 'w') as f:
        json.dump(label_map, f)

    # Get generators
    traingen, valgen = get_training_generator(images,
                                              labels,
                                              config['training']['batch_size'],
                                              config['training']['split'])

    # Setup model
    model = get_model(input_shape=(128,128,1))
    model.summary()
    #opt = SGD(lr=0.001, momentum=0.9)
    opt = Adadelta(epsilon=1e-6, rho=0.99)
    lr_metric = get_lr_metric(opt) # current learning rate metric
    model.compile(optimizer=opt, loss='categorical_crossentropy',
        metrics=['categorical_accuracy', lr_metric])

    # Train model
    cbacks = get_callbacks(subdir, learn_rate=config['training']['learn_rate'],
        verbosity=config['training']['verbosity'],
        save_period=config['training']['save_period'])
    try:
        history = model.fit_generator(traingen,
                                  epochs=config['training']['epochs'],
                                  steps_per_epoch=traingen.n // traingen.batch_size,
                                  verbose=config['training']['verbosity'],
                                  validation_data=valgen,
                                  validation_steps=valgen.n // valgen.batch_size,
                                  callbacks=cbacks)
    except KeyboardInterrupt:
        log.info('Keyboard Interrupt detected')
    finally:
        last_weights = os.path.join(subdir, 'last_weights.h5')
        model.save_weights(last_weights)
        model_json_file = os.path.join(subdir, 'model.json')
        with open(model_json_file, 'w') as f:
            f.write(model.to_json())
    return

if __name__ == '__main__':
    parser = get_parser()
    args = parser.parse_args()
    if args.debug:
        logging.basicConfig(level=logging.DEBUG)
    else:
        logging.basicConfig(level=logging.INFO)
    main(args)
    exit(0)
        
