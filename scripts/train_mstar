#!/usr/bin/env python
# Standard imports
import os
import argparse
import logging
import json
import datetime
import yaml

# External dependencies
import pymongo
import pandas as pd
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from keras.models import Model
from keras.optimizers import SGD, Adadelta
from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras.callbacks import CSVLogger, LearningRateScheduler
from keras.utils import to_categorical

# Local imports
from data.load2 import load_images, get_training_generator
from utils.argutils import valid_path
from aconv import get_aconv
from small import get_model
from training.learning import step_decay, get_lr_metric

DATABASE = 'mstar2'
COLLECTION = 'targets'
SYNTHETICS = 'synthetics'
DEP_ANGLE_TRAIN = '17_DEG'
LABEL_MAP_FILE = 'label_map.json'
DEFAULT_SAVE_PERIOD = 5
DEFAULT_EPOCHS = 50
DEFAULT_BATCH_SIZE = 32
DEFAULT_VERBOSITY = 0
DEFAULT_VALIDATION_SPLIT = 0.1
DEFAULT_LEARN_RATE = 0.001
CHECKPOINT_FILEPATH = "weights.{epoch:02d}.h5"
TRAINING_LOG = 'training.log'
CSV_FILE = 'train_samples.csv'

log = logging.getLogger(__name__)

def get_parser():
    parser = argparse.ArgumentParser(
        description="""Train a Deep Convolutional model to classify the MSTAR
                       dataset""",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('outdir', type=valid_path,
        help="""Output directory for test/training results""")

    parser.add_argument('--weights', type=valid_path, default=None,
        help="""Weights file to load""")

    parser.add_argument('--save-period', dest='save_period', type=int,
        default=DEFAULT_SAVE_PERIOD,
        help="""How often (per epoch) to save model state""")

    parser.add_argument('--epochs', type=int,
        default=DEFAULT_EPOCHS,
        help="""number of training epochs""")

    parser.add_argument('--verbosity', type=int,
        default=DEFAULT_VERBOSITY,
        help="""verbosity mode 0 = silent, 1 = progress bar, 2 = one line per epoch""")

    parser.add_argument('--batch', type=int,
        default=DEFAULT_BATCH_SIZE,
        help="""training batch size""")

    parser.add_argument('--learn-rate', type=float, dest='learn_rate',
        default=DEFAULT_LEARN_RATE,
        help="""Learning rate""")

    parser.add_argument('--split', type=float,
        default=DEFAULT_VALIDATION_SPLIT,
        help="""percentage of validation data to split off of training data""")

    parser.add_argument('--use-synths', action='store_true', dest='use_synths',
        help="""train with synthetic images""")

    parser.add_argument('--prefix', type=str, default=None,
        help="""Prefix to add to created session output subdirectory""")

    parser.add_argument('--max-samples', type=int, dest='max_samples',
        default=None,
        help="""Maximum number of samples to use PER CLASS""")

    parser.add_argument('--max-synth-samples', type=int, dest='max_synth_samples',
        default=None,
        help="""Maximum number of synthetic samples to use PER CLASS""")
    #
    # Database options
    #
    dbgroup = parser.add_argument_group('Database Options')
    dbgroup.add_argument('--database', type=str, default=DATABASE,
        help="""Database to pull training data from""")
    dbgroup.add_argument('--collection', type=str, default=COLLECTION,
        help="""Collection to pull real training samples from""")
    dbgroup.add_argument('--synth-collection', dest='synth_collection', type=str,
        default=SYNTHETICS,
        help="""Collection to pull sythetic training samples from""")
    return parser

def get_callbacks(outdir, learn_rate=0.01, verbosity=0, save_period=5):
    checkpoint = ModelCheckpoint(filepath=os.path.join(outdir, CHECKPOINT_FILEPATH),
                                 monitor='val_loss',
                                 verbose=verbosity,
                                 save_best_only=True,
                                 period=save_period)
    csv_logger = CSVLogger(filename=os.path.join(outdir, TRAINING_LOG),
                           separator=',',
                           append=False)
    return [checkpoint, csv_logger]


def main(args):
    # Make subdirectory for session
    tstamp = datetime.datetime.utcnow()
    subdir = tstamp.strftime("%Y%m%d%H%M%S")
    subdir = '.'.join(('train', subdir))
    if args.prefix:
        subdir = '_'.join((args.prefix, subdir))
    subdir = os.path.join(args.outdir, subdir)
    os.mkdir(os.path.join(args.outdir, subdir), 0o755)
    log.info("Results will be logged to %s" % subdir)

    # Save command line args
    configfile = os.path.join(subdir, 'config.yaml')
    log.info("Saving config to %s" % configfile)
    with open(configfile, 'w') as f:
        yaml.dump(vars(args), f, default_flow_style=False)
    
    # Setup DB connection
    client = pymongo.MongoClient()
    db = client[args.database]

    #
    # Construct dataframe of real training samples
    #
    query = {'depression_angle': DEP_ANGLE_TRAIN}
    cursor = db[args.collection].find(query)
    df = pd.DataFrame(list(cursor))
    if args.max_samples: # Limit smaples per class
        log.info("limiting max real samples per class to %d" % args.max_samples)
        subsets = []
        for name, group in df.groupby('target_class'):
            subsets.append(group.sample(args.max_samples))
        df = pd.concat(subsets)

    #
    # Construct dataframe of synthetic training samples
    #
    if args.use_synths:
        log.info("Loading synthetic samples")
        cursor = db[args.synth_collection].find() # pull all synths
        synth_df = pd.DataFrame(list(cursor))
        if args.max_synth_samples: # Limit smaples per class
            log.info("limiting max synth samples per class to %d" % args.max_synth_samples)
            subsets = []
            for name, group in df.groupby('target_class'):
                subsets.append(group.sample(args.max_synth_samples))
            synth_df = pd.concat(subsets)
        log.debug("Concatenating real and synthetic samples")
        df = pd.concat([df, synth_df], sort=False)

    # Write dataframe to CSV file
    df.to_csv(os.path.join(subdir, CSV_FILE))

    # Load images
    images = load_images(list(df.filename), crop_dim=(128,128))

    # Encode labels - for categogorical crossentropy the labels must be
    # one hot encoded
    classes = df.target_class.unique()
    classes.sort()
    num_classes = len(classes)
    label_map = dict(zip(classes, range(num_classes)))
    labels = df.target_class.apply(lambda x: label_map[x])
    labels = to_categorical(labels, num_classes)
    
    # Write out label map
    with open(os.path.join(subdir, LABEL_MAP_FILE), 'w') as f:
        json.dump(label_map, f)

    # Get generators
    traingen, valgen = get_training_generator(images, labels, args.batch, args.split)

    # Setup model
    # A-ConvNets model
    #model = get_aconv(input_shape=(88, 88, 1))
    model = get_model(input_shape=(128,128,1))
    model.summary()
    #opt = SGD(lr=0.001, momentum=0.9)
    opt = Adadelta(epsilon=1e-6, rho=0.99)
    lr_metric = get_lr_metric(opt) # current learning rate metric
    model.compile(optimizer=opt, loss='categorical_crossentropy',
        metrics=['categorical_accuracy', lr_metric])

    # Train model
    cbacks = get_callbacks(subdir, learn_rate=args.learn_rate,
        verbosity=args.verbosity, save_period=args.save_period)
    try:
        history = model.fit_generator(traingen,
                                  epochs=args.epochs,
                                  steps_per_epoch=traingen.n // traingen.batch_size,
                                  verbose=args.verbosity,
                                  validation_data=valgen,
                                  validation_steps=valgen.n // valgen.batch_size,
                                  callbacks=cbacks)
    except KeyboardInterrupt:
        log.info('Keyboard Interrupt detected')
    finally:
        last_weights = os.path.join(subdir, 'last_weights.h5')
        model.save_weights(last_weights)
        model_json_file = os.path.join(subdir, 'model.json')
        with open(model_json_file, 'w') as f:
            f.write(model.to_json())
    return

if __name__ == '__main__':
    import traceback
    logging.basicConfig(level=logging.INFO)
    parser = get_parser()
    args = parser.parse_args()
    try:
        main(args)
    except Exception as err:
        print(err)
        print(traceback.format_exc())
        exit(1)
    else:
        exit(0)
        
