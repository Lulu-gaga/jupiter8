#!/usr/bin/env python
# Standard imports
import os
import sys
import argparse

# External dependencies
import numpy as np
import matplotlib.pyplot as plt
from keras.applications.resnet50 import ResNet50
from keras.layers import Input, Dense, GlobalAveragePooling2D
from keras.models import Model
from keras.callbacks import ModelCheckpoint

# Local imports
from data.load import get_predict_generator
from analysis.visualize import plot_history
from utils.argutils import valid_path

IMG_SHAPE = (128, 128, 1)
DEFAULT_NUM_SAMPLES = 200
DEFAULT_SAVE_PERIOD = 5
DEFAULT_EPOCHS = 20
DEFAULT_BATCH_SIZE = 32
DEFAULT_VERBOSITY = 0
DEFAULT_VALIDATION_SPLIT = 0.1

NUM_CLASSES = 3

def get_parser():
    parser = argparse.ArgumentParser()
    parser.add_argument('outdir', type=valid_path,
        help="""Output directory for training results""")
    parser.add_argument('weights', type=valid_path,
        help="""Weights file to load""")
    parser.add_argument('--save-period', dest='save_period',
        default=DEFAULT_SAVE_PERIOD,
        help="""How often (per epoch) to save model state""")
    parser.add_argument('--samples', type=int,
        default=DEFAULT_NUM_SAMPLES,
        help="""number of samples PER CLASS to train with""")
    parser.add_argument('--epochs', type=int,
        default=DEFAULT_EPOCHS,
        help="""number of training epochs""")
    parser.add_argument('--verbosity', type=int,
        default=DEFAULT_VERBOSITY,
        help="""verbosity mode 0 = silent, 1 = progress bar, 2 = one line per epoch""")
    parser.add_argument('--batch', type=int,
        default=DEFAULT_BATCH_SIZE,
        help="""training batch size""")
    parser.add_argument('--split', type=float,
        default=DEFAULT_VALIDATION_SPLIT,
        help="""percentage of validation data to split off of training data""")
    return parser

def main(args):
    predict_gen = get_predict_generator()
    base_model = ResNet50(weights=args.weights, include_top=False, input_shape=IMG_SHAPE, classes=3,
        pooling='avg')
    x = base_model.output
    predictions = Dense(3, activation='softmax')(x)
    model = Model(inputs=base_model.input, outputs=predictions)
    model.summary()
    num_samples = len(predict_gen.filenames)
    res = model.evaluate_generator(predict_gen, steps=num_samples)
    return res

if __name__ == '__main__':
    parser = get_parser()
    args = parser.parse_args()
    res = main(args)
    print(res)
    sys.exit(0)
