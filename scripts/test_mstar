#!/usr/bin/env python
# Standard imports
import os
import argparse
import logging
import json
import datetime

# External dependencies
import pymongo
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Local imports
from aconv import get_aconv
from small import get_model
from data.load2 import load_images, get_test_generator
from utils.argutils import valid_path
from analysis.confusion import cmtable

DATABASE = 'mstar2'
COLLECTION = 'targets'
DEP_ANGLE_TEST = '15_DEG'
LABEL_MAP = 'label_map.json'
WEIGHTS = 'weights.%02d.h5'
CSV_FILE = 'test_samples.csv'

log = logging.getLogger(__name__)

def get_parser():
    parser = argparse.ArgumentParser(
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    
    parser.add_argument('outdir', type=valid_path,
        help="""Output directory for test/training results""")

    parser.add_argument('weightsnum', type=int,
        help="""Weights file to load""")

    parser.add_argument('--batch', type=int, default=32,
        help="""Batch size""")

    parser.add_argument('--max-samples', type=int, dest='max_samples',
        default=None,
        help="""Maximum number of samples to use PER CLASS""")
    return parser


def main(args):
    # Setup DB connection
    client = pymongo.MongoClient()
    db = client[DATABASE]
    collection = db[COLLECTION]

    # Query training data
    query = {'depression_angle': DEP_ANGLE_TEST}
    cursor = collection.find(query)

    # Construct data frame
    df = pd.DataFrame(list(cursor))

    # Limit samples used per class
    if args.max_samples:
        grouped = df.groupby('target_class')
        subsets = []
        for name, group in grouped:
            subsets.append(group.sample(args.max_samples))
        df = pd.concat(subsets)

    # Load images
    log.info("loading images")
    images = load_images(list(df.filename), crop_dim=(128,128))
    log.info("loaded %d images" % len(images))

    # Setup labels
    log.info("configuring labels")
    with open(os.path.join(args.outdir, LABEL_MAP), 'r') as f:
        label_map = json.load(f)
    labels = df.target_class.apply(lambda x: label_map[x])
    np.savetxt(os.path.join(args.outdir, 'labels.log'), labels, fmt='%d', delimiter=',')
    log.info("loaded %d labels" % len(labels))

    # Get data generator
    datagen = get_test_generator(images, labels, args.batch)

    # Setup model
    #model = get_aconv((88, 88, 1))
    model=get_model(input_shape=(128,128,1))
    log.info("loading model weights")
    model.load_weights(os.path.join(args.outdir,WEIGHTS % args.weightsnum))
    model.summary()

    # Evaluate
    log.info("predicting on samples")
    y = model.predict_generator(datagen, steps=datagen.n // args.batch + 1)
    predictions = np.argmax(y, axis=1)
    np.savetxt(os.path.join(args.outdir, 'predictions.log'), predictions, fmt='%d', delimiter=',')
    print(confusion_matrix(labels, predictions, labels=range(10)))

    ax = cmtable(labels, predictions, label_map)
    plt.show()

    return

if __name__ == '__main__':
    logging.basicConfig(level=20)
    parser = get_parser()
    args = parser.parse_args()
    try:
        main(args)
    except Exception as err:
        log.error(err, exc_info=True)
        exit(1)
    else:
        exit(0)
